{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignment 4.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"YJwnA6OrwAGk","colab_type":"text"},"cell_type":"markdown","source":["---\n","\n","_You are currently looking at **version 1.1** of this notebook. To download notebooks and datafiles, as well as get help on Jupyter notebooks in the Coursera platform, visit the [Jupyter Notebook FAQ](https://www.coursera.org/learn/python-data-analysis/resources/0dhYG) course resource._\n","\n","---"]},{"metadata":{"id":"CAqX4MdawAGl","colab_type":"code","colab":{}},"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from scipy.stats import ttest_ind"],"execution_count":0,"outputs":[]},{"metadata":{"id":"msMVvlxWwAGs","colab_type":"text"},"cell_type":"markdown","source":["# Assignment 4 - Hypothesis Testing\n","This assignment requires more individual learning than previous assignments - you are encouraged to check out the [pandas documentation](http://pandas.pydata.org/pandas-docs/stable/) to find functions or methods you might not have used yet, or ask questions on [Stack Overflow](http://stackoverflow.com/) and tag them as pandas and python related. And of course, the discussion forums are open for interaction with your peers and the course staff.\n","\n","Definitions:\n","* A _quarter_ is a specific three month period, Q1 is January through March, Q2 is April through June, Q3 is July through September, Q4 is October through December.\n","* A _recession_ is defined as starting with two consecutive quarters of GDP decline, and ending with two consecutive quarters of GDP growth.\n","* A _recession bottom_ is the quarter within a recession which had the lowest GDP.\n","* A _university town_ is a city which has a high percentage of university students compared to the total population of the city.\n","\n","**Hypothesis**: University towns have their mean housing prices less effected by recessions. Run a t-test to compare the ratio of the mean price of houses in university towns the quarter before the recession starts compared to the recession bottom. (`price_ratio=quarter_before_recession/recession_bottom`)\n","\n","The following data files are available for this assignment:\n","* From the [Zillow research data site](http://www.zillow.com/research/data/) there is housing data for the United States. In particular the datafile for [all homes at a city level](http://files.zillowstatic.com/research/public/City/City_Zhvi_AllHomes.csv), ```City_Zhvi_AllHomes.csv```, has median home sale prices at a fine grained level.\n","* From the Wikipedia page on college towns is a list of [university towns in the United States](https://en.wikipedia.org/wiki/List_of_college_towns#College_towns_in_the_United_States) which has been copy and pasted into the file ```university_towns.txt```.\n","* From Bureau of Economic Analysis, US Department of Commerce, the [GDP over time](http://www.bea.gov/national/index.htm#gdp) of the United States in current dollars (use the chained value in 2009 dollars), in quarterly intervals, in the file ```gdplev.xls```. For this assignment, only look at GDP data from the first quarter of 2000 onward.\n","\n","Each function in this assignment below is worth 10%, with the exception of ```run_ttest()```, which is worth 50%."]},{"metadata":{"id":"AKOaYeqlwAGw","colab_type":"code","colab":{}},"cell_type":"code","source":["# Use this dictionary to map state names to two letter acronyms\n","states = {'OH': 'Ohio', 'KY': 'Kentucky', 'AS': 'American Samoa', 'NV': 'Nevada', 'WY': 'Wyoming', 'NA': 'National', 'AL': 'Alabama', 'MD': 'Maryland', 'AK': 'Alaska', 'UT': 'Utah', 'OR': 'Oregon', 'MT': 'Montana', 'IL': 'Illinois', 'TN': 'Tennessee', 'DC': 'District of Columbia', 'VT': 'Vermont', 'ID': 'Idaho', 'AR': 'Arkansas', 'ME': 'Maine', 'WA': 'Washington', 'HI': 'Hawaii', 'WI': 'Wisconsin', 'MI': 'Michigan', 'IN': 'Indiana', 'NJ': 'New Jersey', 'AZ': 'Arizona', 'GU': 'Guam', 'MS': 'Mississippi', 'PR': 'Puerto Rico', 'NC': 'North Carolina', 'TX': 'Texas', 'SD': 'South Dakota', 'MP': 'Northern Mariana Islands', 'IA': 'Iowa', 'MO': 'Missouri', 'CT': 'Connecticut', 'WV': 'West Virginia', 'SC': 'South Carolina', 'LA': 'Louisiana', 'KS': 'Kansas', 'NY': 'New York', 'NE': 'Nebraska', 'OK': 'Oklahoma', 'FL': 'Florida', 'CA': 'California', 'CO': 'Colorado', 'PA': 'Pennsylvania', 'DE': 'Delaware', 'NM': 'New Mexico', 'RI': 'Rhode Island', 'MN': 'Minnesota', 'VI': 'Virgin Islands', 'NH': 'New Hampshire', 'MA': 'Massachusetts', 'GA': 'Georgia', 'ND': 'North Dakota', 'VA': 'Virginia'}"],"execution_count":0,"outputs":[]},{"metadata":{"id":"H_21wHwbq3KO","colab_type":"code","outputId":"4d2d569d-ecd1-47cd-bdd0-85137bc8020a","executionInfo":{"status":"ok","timestamp":1543290709315,"user_tz":-420,"elapsed":32288,"user":{"displayName":"Quân Hoàng Bà","photoUrl":"","userId":"13292419351910612628"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/data')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /data\n"],"name":"stdout"}]},{"metadata":{"id":"i3rvNP7M9OyG","colab_type":"code","outputId":"b7b72536-27a4-4f33-f158-54a706826aef","executionInfo":{"status":"ok","timestamp":1543290404894,"user_tz":-420,"elapsed":9241,"user":{"displayName":"Quân Hoàng Bà","photoUrl":"","userId":"13292419351910612628"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"cell_type":"code","source":["!pip install xlrd"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting xlrd\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/e6/e95c4eec6221bfd8528bcc4ea252a850bffcc4be88ebc367e23a1a84b0bb/xlrd-1.1.0-py2.py3-none-any.whl (108kB)\n","\u001b[K    100% |████████████████████████████████| 112kB 2.5MB/s \n","\u001b[?25hInstalling collected packages: xlrd\n","Successfully installed xlrd-1.1.0\n"],"name":"stdout"}]},{"metadata":{"id":"YKT2_yD6RL98","colab_type":"code","colab":{}},"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import re \n","from scipy.stats import ttest_ind\n","def prettity(x):\n","  m = re.search('\\[',x)\n","  if m:\n","    x=x[:m.start()]\n","  m=re.search('\\(',x)\n","  if m:\n","    x=x[m.start():]\n","\n","  return x\n","def prettitya(x):\n","  m = re.search('\\(',x)\n","  if m:\n","    x=x[:m.start()]\n","  return x\n","file = open('/data/My Drive/data/university_towns.txt', 'r') \n","pds=pd.Series([0,0],index=['a','b'])\n","n=\"\"\n","for i in file:\n","  if('[edit]' in i):\n","    n=i\n","    continue\n","  else:\n","    a=pd.Series([i,n],index=['a','b'])\n","    pds=pd.concat([pds,a],axis=1,join='inner')\n","pds"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7WHAMhm5wAG0","colab_type":"code","outputId":"97bd2b9b-9a51-4632-a5d5-18c23a087818","executionInfo":{"status":"ok","timestamp":1543290745833,"user_tz":-420,"elapsed":1012,"user":{"displayName":"Quân Hoàng Bà","photoUrl":"","userId":"13292419351910612628"}},"colab":{"base_uri":"https://localhost:8080/","height":80}},"cell_type":"code","source":["def get_list_of_university_towns():\n","    data = []\n","    State = None\n","    State_RegionNames = []\n","    with open('/data/My Drive/data/university_towns.txt') as file:\n","        for line in file:\n","            thisLine = line[:-1]\n","            if thisLine[-6:] == '[edit]':\n","                State = thisLine[:-6]\n","                continue\n","            if '(' in line:\n","                RegionName = thisLine[:thisLine.index('(')-1]\n","                State_RegionNames.append([State,RegionName])\n","            else:\n","                RegionName = thisLine\n","                State_RegionNames.append([State,RegionName])\n","            data.append(thisLine)\n","    df = pd.DataFrame(State_RegionNames,columns = ['State','RegionName'])\n","    return df\n","  \n","get_list_of_university_towns()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>State</th>\n","      <th>RegionName</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>100</th>\n","      <td>Illinois</td>\n","      <td>Evanston</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        State RegionName\n","100  Illinois   Evanston"]},"metadata":{"tags":[]},"execution_count":16}]},{"metadata":{"id":"Ir-6ASykwAG4","colab_type":"code","colab":{}},"cell_type":"code","source":["import pandas as pd\n","def get_recession_start():\n","    quarter = pd.read_excel('/data/My Drive/data/gdplev.xls', skiprows=219)\n","    quarter = quarter[['1999q4', 9926.1]]\n","    quarter.columns = ['Quarter','GDP']\n","    for i in range(2, len(quarter)):\n","        if (quarter.iloc[i-2][1] - quarter.iloc[i-1][1])>0 and (quarter.iloc[i-1][1] - quarter.iloc[i][1])>0:\n","            return quarter.iloc[i-2][0]\n","\n","get_recession_start()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"SrcvdwJ1wAG7","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_recession_end():\n","    quarter = pd.read_excel('/data/My Drive/data/gdplev.xls', skiprows=219)\n","    quarter = quarter[['1999q4', 9926.1]]\n","    quarter.columns = ['Quarter','GDP']\n","    recession_start = get_recession_start()\n","    recession_start_index = quarter[quarter['Quarter'] == recession_start].index.tolist()[0]\n","    quarter=quarter.iloc[recession_start_index:]\n","    for i in range(2, len(quarter)):\n","        if (quarter.iloc[i-2][1] - quarter.iloc[i-1][1])<0 and (quarter.iloc[i-1][1] - quarter.iloc[i][1])<0:\n","            return quarter.iloc[i][0]\n","get_recession_end()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"l5rSOa95wAG_","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_recession_bottom():\n","    '''Returns the year and quarter of the recession bottom time as a \n","    string value in a format such as 2005q3'''\n","    quarter = pd.read_excel('/data/My Drive/data/gdplev.xls', skiprows=219)\n","    quarter = quarter[['1999q4', 9926.1]]\n","    quarter.columns = ['Quarter','GDP'] \n","    recession_start = get_recession_start()\n","    recession_start_index = quarter[quarter['Quarter'] == recession_start].index.tolist()[0]\n","    recession_end = get_recession_end()\n","    recession_end_index = quarter[quarter['Quarter'] == recession_end].index.tolist()[0]\n","    quarter=quarter.iloc[recession_start_index:recession_end_index+1]\n","    recession_bottom = quarter['GDP'].min()\n","    recession_bottom_index = quarter[quarter['GDP'] == recession_bottom].index.tolist()[0]-recession_start_index\n","    return quarter.iloc[recession_bottom_index]['Quarter']\n","get_recession_bottom()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"x_w_XrEGwAHH","colab_type":"code","colab":{}},"cell_type":"code","source":["def new_col_names():\n","    years = list(range(2000,2017))\n","    quars = ['q1','q2','q3','q4']\n","    quar_years = []\n","    for i in years:\n","        for x in quars:\n","            quar_years.append((str(i)+x))\n","    return quar_years[:67]\n","new_col_names()\n","def convert_housing_data_to_quarters():\n","    data = pd.read_csv('/data/My Drive/data/City_Zhvi_AllHomes.csv')\n","    data.drop(columns=['Metro','CountyName','RegionID','SizeRank'],axis=1,inplace=True)\n","    data['State'] = data['State'].map(states)\n","    data.set_index(['State','RegionName'],inplace=True)\n","    col = list(data.columns)\n","    col = col[0:45]\n","    data.drop(col,axis=1,inplace=True)\n","\n","    #qs is the quarters of the year\n","    qs = [list(data.columns)[x:x+3] for x in range(0, len(list(data.columns)), 3)]\n","    \n","    # new columns\n","    column_names = new_col_names()\n","    for col,q in zip(column_names,qs):\n","        data[col] = data[q].mean(axis=1)\n","        \n","    data = data[column_names]\n","    return data\n","convert_housing_data_to_quarters()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"rI6kSf1awAHN","colab_type":"code","colab":{}},"cell_type":"code","source":["def run_ttest():\n","    '''First creates new data showing the decline or growth of housing prices\n","    between the recession start and the recession bottom. Then runs a ttest\n","    comparing the university town values to the non-university towns values, \n","    return whether the alternative hypothesis (that the two groups are the same)\n","    is true or not as well as the p-value of the confidence. \n","    \n","    Return the tuple (different, p, better) where different=True if the t-test is\n","    True at a p<0.01 (we reject the null hypothesis), or different=False if \n","    otherwise (we cannot reject the null hypothesis). The variable p should\n","    be equal to the exact p value returned from scipy.stats.ttest_ind(). The\n","    value for better should be either \"university town\" or \"non-university town\"\n","    depending on which has a lower mean price ratio (which is equivilent to a\n","    reduced market loss).'''\n","    \n","    return \"ANSWER\""],"execution_count":0,"outputs":[]}]}